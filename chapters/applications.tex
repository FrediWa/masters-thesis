Fourier series were originally motivated by a physics differential equation and even if it still is of great help for mathematicians solving particular types of differential equations, the true importance comes from the vast applicability of Fourier analysis combined with the significant performance improvement of the FFT. The Fourier transform allows the conversion between time and frequency domain which makes manipulation and analysis of arbitrary signals practical. 

Signals are all around us, hidden in every day life. There are the more obvious ones like various forms of sound and wireless communications that everyone with a mobile phone utilizes multiple times per day. There are also less obvious ones like images and really any kind of data. This chapter will explore some of the applications of the Fourier transform to emphasize the importance of the idea. \todo{This whole section shouldn't need a source}

\subsection{Wireless communication}
A lot of modern standards for wireless communication, like IEEE 802.11 WLAN (colloqially Wi-Fi), rely on Orthogonal Frequency Division Multiplexing (OFMD) \todo{https://warse.org/IJWCNT/static/pdf/file/ijwcnt04332014.pdf}. In short, it's a way to pack discrete data (consecutive bits for example) compactly into a time-domain signal of orthogonal sinusoids. Orthogonal means that the signals do not mess with eachother and that integrating the product of several sinusoids over the period gives 0. This happens to be case when the angular frequency of two signals are two different integers. \todo{Source? Even though some literature and sources just take this for granted. When n = m, the integral is $sin^2(nx)$ which will always be positive so the integral is not 0.}

OFDM needs a modulation scheme to convert data into sinusoid components. Quadrature Phase Shift Keying is one popular method and it transforms the 4 combinations of 1's and 0's into the 4 roots of unity. 00 becomes 1, 01 becomes i, 10 becomes -1 and 11 becomes -i. These values are encoded in so called subcarriers which are then turned into the time-domain using the IFFT in preparation for transmission. An FFT on the receiver's end transform the transmission to frequency-domain for demodulation. The frequencies and phases can then be extracted using the inverse of the modulation scheme to get the original data that made up the transmitted time-domain signal.\todo{Source?}

% \subsection{Spectroscopy}

\subsection{Images are signals}
Just like data is encoded in a signal for wireless communication, a signal can be also used to represent an image. Optical illusions show that human  eyes are not great at spotting certain information. The lossy image format JPEG uses this to it's advantage, getting rid of small color changes, while keeping high contrast brightness changes.  

The Discrete Cosine Transformed  is typically the preferred method for it's properties aligning closer to the goals of image processing and because it's simpler to compute, being limited to real inputs and outputs \todo{does the DCT need a source?}. However \todo{there is one paper that uses the FFT specifically, need to look into what makes it special there, or not?}

\subsection{Multiplication}\todo{This whole section may be wrong as I delved deeper into the FFT algorithm after I wrote this}
A more novel example of an application of the FFT is the multiplication of two numbers. As previously mentioned, multiplication is an algorithm that has a time complexity of $O(n^2)$ because each digit needs to be multiplied with every other digit. Using a fast Fourier transform, two numbers can be multiplied in $O(nlogn)$. The general gist of the method is to represent both number as a polynomial, sampling them at a number of points (a number which must be large enough to uniquely define the polynomial), multiplying those points together, finding a new unique polynomial for the new set of points and converting the polynomial back to a number. That resulting number is the product of the two input numbers \cite{Reducible2020}.

Sampling is where the FFT enters the picture. The last step of the multiplication method requires a unique polynomial so $N \geq d+1$ samples are necessary where $d = n-1$, the highest degree in the polynomial or one less than the number of digits in the inputs. For every sample, the entire polynomial is required which means that the sampling part takes $O(n^2)$ computations anyway. The FFT accelerates this by utilizing the symmetry in the roots of unity to take the samples in $O(nlogn)$. This procedure is done for both input numbers, contributing 2nlogn operations or $(nlogn)$ \cite{Reducible2020}.

The next step is a component wise multiplication of the transforms, which takes $O(n)$ operations. The last step involves taking the inverse to get the result. This is done with the inverse FFT, which also runs in $O(nlogn)$ \cite{Reducible2020}.

All in all the multiplication algorithm takes $4N + 3(nlogn)$ operations which is $O(nlogn)$. Even though this seems like a very convoluted process, for large numbers, this turns advantageous at inputs of length around 200 \cite{Emerencia2007}. This means this method is practical in cryptography where it's somewhat common to multiply extremely large integers. \todo{Do I need a source? Or is it sufficiently well known that RSA can have hundreds of digits long integers.}



\subsection{Music and audio processing}