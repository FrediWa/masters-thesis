The purpose of this work is to study a short-time Fourier transform for its viability as a pitch detector. The FFT for transforming a waveform into the frequency-domain has been introduced and this chapter will focus on the properties of sound, introduce some musical terminology and how the FFT can be used to detecting pitches from a time-domain signal

\subsection{Basics of sound}
Sound physically is pressure propagation through a medium, a vibration which some things can produce and some things can pick up. The word sound can be used to describe both the propagation itself, but also the phenomenom we feel when our ears react to the propagation. Headphones, strings of a guitar and vocal chords (together with the lungs) are some of the things that can produce sound and microphones and membranes in ears are things that can pick up sound. The propagation will have a certain strength at some point in the medium it travels through, which can be measured. This allows us to model sound as pressure over time. At time $t$ the pressure at some point can be denoted as $f(t)$. This function over time can also be called a signal.

% source: https://api.pageplace.de/preview/DT0400.9781292055152_A24617782/preview-9781292055152_A24617782.pdf The Science of Sound - Rossing, Moore, Wheeler

One of the simplest ways to generate a sound is connecting a speaker to a device that generates an analog current in the shape of a wave. The current moves an electromagnet which moves a membrane at the same frequency as the generator causing the pressure difference around the membrane. This membrane displaces air at some rate which ears pick up as sound. For example, if the generator produces a 440Hz signal, the speaker's membrane will displace air at the same 440Hz. This displacement is propagated over air and is sensed by our ear as a pitch we call A4. Interestingly, the purity of the signal makes it sound harsh and it is noise in the signal that gives warmth and beauty to the note. This can be observed in figure \ref{fig:pianoWave} where the signal (sound made by an electric piano) largely matches a pure A4 (440Hz sinusoid) but not quite. The "warmth and beauty" is formally called timbre and it allows us to distinguish an A4 on a piano from an A4 on a guitar even though both are A4 sounds.

%https://eprints.hud.ac.uk/id/eprint/17816/1/Final_Thesis_-_November_2012.pdf why is this here?

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{./images/piano_wave.png}
    \caption{A slice of the recording of a A4 note played on a Yamaha electric piano (blue solid). The signal contains a lot of irregularity but also clear regularity which becomes more evident when layering a pure A4 note on top (red dashed).\label{fig:pianoWave}}
\end{figure}

The characteristics of the sound being played becomes even more evident when it's converted to the frequency-domain through the FFT as shown in figure \ref{fig:pianoFreq}. It reveals that, not only is the sound definitely a A4 note due to the overwhelming amount of the 440Hz signal present in the sound, but also what other frequencies contribute to the sound of that piano. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{./images/piano_freq.png}
    \caption{The frequency-domain clearly reveals without any guesswork that the signal is a 440Hz signal with some sort of noise/disturbance. The red vertical line is placed at $x=440$ to show that the peak is the expected 440Hz frequency component.\label{fig:pianoFreq}}
\end{figure}

The spectrum also reveals that the particular electric piano that was used to create the sound lacks harmonic content. This is not the case for  

\subsection{Music 101} 
Music terminology can be hard to define without circular definitions. There's also quite a bit of overlap with the same word meaning different things depending on context. A possible starting point would be defining the octave which is the difference between two sounds where one sound has twice the frequency of the other and is the largest interval in western music. If a sound oscillates at 440Hz, the sounds corresponding to 220Hz or 880Hz are an octave away.


\subsubsection{Octaves}
The name octave comes from the fact that it contains 7 distinct basenotes (the 8th forming the octave). This is called a heptatonic scale and when it sounds "nice" it's called a diatonic scale (formally, it requires a certain structure of intervals). Diatonic scales include the familiar major (happy) and minor (sad) scales. The history and reasoning for the heptatonic scales in western music theory is not relevant for the work, but the point is that it's not the only taxonomy for notes. 

Because the diatonic scales require a certain structure of intervals, the octave needs to be divided further into what is called semitones. Semitone is typically defined as the smallest interval in western music and is typically 1/12th of an octave. Semitone is sometimes also used to talk about, not the interval, but the sounds that bound the interval. This non-interval semitone is sometimes also called a pitch and sometimes note.

\subsubsection{Notes and note names}
Whle octave is used to describe an interval of 2 notes, we can define a first, second third etc. octave if we start from somewhere. A common way to label musical pitches is to use a letter to represent the note name and a number to indicate which octave it belongs to. This system is called Scientific Pitch Notation (SPN) and is used in Western music notation." It starts, not on an A, but on a C and depending on the convention used ends in either a B or an H. Figure \ref{fig:noteScale} shows the octave bounds and how notes are ordered within the octave.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{./images/noteScale.png}
    \caption{Notes of an octave. Base notes are marked with purple. \label{fig:noteScale}}
\end{figure}

For example, the note which has twice the frequency of A4 forms an octave with A4. As the frequency is doubled, the other note is one octave higher than A4 so it is given the name A5. If the frequency is halved, it is named A3. 

In western music, tuning typically starts from A4, which is defined in this system to have a frequency of 440Hz, and every other note is derived from there. Tuning in this context essentially means finding the frequencies of the other notes so that everything sounds "right" (to avoid getting into unnecessary amount of music theory). A3, an octave lower than A4, will be half of 440Hz or 220Hz and A5, an octave higher, will be double of 440Hz or 880Hz. The frequency for the other pitches can be approximated with equal temperament tuning, meaning that all semitones are equally spaced within octaves. This means that the frequency of any one pitch is exactly $2^{1/12}$ times the frequency of the previous one. Sounds are given identifiers as a letter-number pair based on the octave they are in and their placement within the octave. 

\subsubsection{Flats and sharps}
Flats and sharps are augmented notes. Sharps are denoted with a $\#$ symbol and flats with a $\flat$ symbol. Both flats and sharps are one semitone above the augmented note meaning there is quite a bit of overlap. For instance, a D$\#$4 has the exact same pitch as a E$\flat$4. The key difference between D$\#$4 and E$\flat$4 is context. It should be obvious, even to a person with no musical training, that when "playing in D-sharp", there should be a D-sharp and not an E-flat even though they are effectively the same note. In some situations there is also the concept of a "double sharp" and "double flat".    

\subsubsection{Terminology for readers}
The base notes will be defined according to German/European music theory as \[C, D, E, F, G, A, H\] and B being $H\flat$. In this work, the term "note" will be used to describe the 12 distinct sounds within the octave because for the pitch detector, discerning the base notes and the sharps and flats isn't necessary. Pitch will be used to describe the measurement/perception of how low or high a sound is.

\subsubsection{MIDI numbers}
For humans, it is easy to use the SPN labels for notes. C4 is a simple pair of a letter and a number and gives users just enough information to know what it refers to. Computers on the other hand are different and instead have it much harder to analyze, say, "A4" even compared to the pure frequencies. However the frequency value is often unnecessary because we usually only care about a discrete set of frequencies corresponding to notes, not all possible real valued frequencies. 
Musical Instrument Digital Interface (MIDI) is a technical standard for protocols, connectors and interfaces relating to music and provides among other things, simple integers for labeling notes. For instance, the first sound on a piano, A0, has the MIDI number 21. 


% \subsubsection{Pitch perception}
% Even though we can clearly define any one pitch to be some frequency, human ears are more complex. 
% What do I want to establish in this chapter????
% What is pitch perception at a high level
% 
\subsection{Fundamental frequency estimation}
Even though we can define a pitch to be a precise frequency, pitch is more of a sensation perceived by ears and we can distinguish pitch from soundwaves that are not simple oscillations. As shown in \ref{fig:pianoWave}, what we perceive as an A4 can contain a lot of noise. As it can be difficult to strictly define pitch, pitch detection may also be hard to define. Pitch is however strongly tied to the fundamental frequency, which is the lowest perceived harmonic of a signal. Even though the timbre of a piano and a guitar are vastly different, which allows us to hear a difference between the two, if they play the same note, the fundamental frequency will be the same. 

From a computational perspective, a possible definition is that pitch detection is the identification of the fundamental frequency and which is the corresponding note (because the sound names are more valuable to musicians than the frequencies expressed as hertz). 

The fundamental frequency is typically the lowest frequency of a signal, but in some cases we can hear a fundamental frequency that isn't actually a part of the signal, a phenomenon appropriately called the missing fundamental. \todo{source: Gotsopoulos}. An example of this would be telephones, that could not record below 300Hz. Male voices would still sound male, even though their fundemental frequency of around 150Hz was completely missing \todo{Mather 2006}.

\subsubsection{The missing fundamental}
The missing fundamental is in a sense an audiotory illusion. It's a result of the constructive interference of harmonics that causes peaks with a period that is the greatest common divisor of the angular frequencies of the harmonics. This is shown in figure \ref{fig:missingfund} where the sum of several waves produce a wave with a prominently 110Hz oscillation. Even though the lowest frequency component is 220Hz, the prominent 110Hz oscillation is what we feel and hear.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{./images/missingfund.png}
    \caption{Constructive interference of harmonics creates the missing fundemental. Even though the blue signal only contains sinusoids with frequencies greater or equal to 220Hz, the most prominent part of the final signal has a period of 110Hz. \label{fig:missingfund}}
\end{figure}

Pipe organs use this to their advantage. Creating extremely low sounds requires enormous pipes. Taking advantage of the missing fundamental allows relatively smaller organs to produce low sounds.
% 0:56; https://www.youtube.com/watch?v=Sn07AMCfaAI&t=1169s

\subsection{Pitch detection methods}
Pitch detection methods typically fall into one of 3 categories: time-domain, frequency-domain and statistical/machine learning methods. Some common time-domain methids include zero-crossing rate and autocorrelation. Zero-crossing rate, is a method in which frequencies are extracted by keeping track of the rate at which the signal crosses the y-axis. This has shown to be effective for certain applications but falls short in other tasks \todo{Source???}. Another is autocorrelation, in which the signal is compared to itself with a different phase. 

This thesis will focus on the frequency-domain approaches, where a fourier transform is used to create a representation of the sound that is easier to analyze and process.

\subsection{Transforming to frequency-domain}
The first step in detecting pitch using a frequency-domain approach is to transform the original signal into the frequeny-domain and there are multiple ways to achieve that. The DFT and FFT were already introduced in the introductory fourier analysis chapter and as already established, some FFT should be used as the transformer as the DFT has terrible performance. How to apply the FFT isn't yet clear however, as the background used dummy data to demonstrate the algorithm and its performance. Disregarding the question of how to process binary audio files, even if the data was decoded, how should the data be processed? If the whole audio signal is given to the FFT, the spectrum will be a complete mess because real music/song contains more than a single note or chord.

\subsubsection{Short-time Fourier Transform}
The short-time Fourier Transform (STFT), sometimes also called the windowed Fourier transform, is a method of analysing a signal by looking at parts of the signal. The STFT is typically defined very similarly to the FT/DFT only with a windowing function applied. The continuous STFT is usually defined like the following.
$$S(x(t), \tau, \omega ) = \int_{-\infty}^{\infty} x(t)W(t-\tau)e^{-i\omega t}$$

where $x(t)$ is the input signal $\tau$ is the center of the window function $W$. The window function may be freely chosen and examples include Gaussian window and Hamming window. Conceptually this is computing the transform for a signal $S_2$ that is some signal $S$ with a windowing function applied so that outside the window, the signal has 0 value. Afterwards, the window is moved and the "next" value is computed. The discrete STFT conversely computes the DFT for just some chunk of a discrete signal. Like the STFT, it then moves the window over some number of steps (called the hop length in discrete context) and repeats. If the hop length is shorter than the window size, the windows will overlap.

This sliding window introduces a new dimension to the output data, a time axis. This data constitutes a spectrogram and the typical way to visualize this data is with a heat map by compressing the 2D of the frequency-domain to a column where intensity represents the magnitude value for a frequency. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{./images/stft.png}
    \caption{High level overview of the SFTF. The STFT performs the DFT for every window which results in a spectrogram. For ease of visualization, the frequency-domain is typically visualized using color intensity for the magnitude. \label{fig:stft}}
\end{figure}

It's worth pointing out that the STFT is not a special kind of fourier transform, but rather a method of applying the DFT/FFT (or contiunuous FT). This means it has the same behaviour but also that it has the problems which will be introduced shortly. 

For real-time processing, it's both imperetive and a bit redundant to talk about the STFT because the entirety of the signal will never be available so there is no need for windowing. The data will be available as a stream (or chunks) and an FFT can be performed once a buffer is filled, after which the buffer is cleared. The buffer serves as the window so as long as the transform is computed sufficiently often (which needs to be done either way in real-time applications) the method in a sense implements a discrete STFT. The overlap, which is determined by the hop length, doesn't seem to be of much importance for the purpose of the estimation the fundamental frequency and makes little sense when the data becomes available with time. The overlap is important when considering note lengths and separation of repeated semitones \cite{Evans2012}, but does not seem to be a concern for just pitch detection. The pitch detector will thus have a hop length the size of the window, or in other words, no window overlap.

\subsection{Picking peaks from the spectrum}
The Fourier transform is used to get the spectrum data, which can be used to plot which frequencies are part of the original signal. These frequencies that are part of the signal are called partials. Intuitively, for a lot of signals, anyone can look at the frequency-magnitude plot and point to a peak and claim that it's the fundamental frequency and probably be right. Perhaps they are looking for some periodicity with the largest peaks (if there are multiple) and choosing out of those the peak that has the lowest frequency. This is a very straightforward approach and correct for some signals. 

% HPS
\subsubsection{Harmonic Product Spectrum}
Harmonic Product Spectrum (HPS) is an algorithm that almost does the intuitive approach. 
$$Y = \prod_{r=2}^R X_r$$ where $X_r$ denotes the signal $X$ downsampled by $r$. Simply put, the HPS iteratively multiplies downsampled version of a signal. Downsampling the frequency-domain effectively compresses the frequency-axis by the downsampling rate which means that previously integer multiplies of some value $kN$ are put in the same bins as $N$. When all the downsampled frequency-domain signals are multiplied together, correlated frequencies, or harmonics, form a very sharp peak \cite{McLeod2008}. This peak can be picked out with a max function.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{./images/hps.png}
    \caption{Plot of signal and downsampled versions of it and the final HPS. Harmonics align and form a peak. \label{fig:hps}}
\end{figure}

HPS is a good algorithm because it's simple and easy to compute making it suitable for real-time analysis. It also has the ability to 

The biggest problem with the HPS is that it does not perform well in the case of lacking harmonics \cite{McLeod2008}. This likely won't be a problem because human voices should be rich in harmonic content. In the more extreme case where noise is dominating, the system would still compute the HPS and one largest peak would form somewhere, resulting in garbage data. A possible way to mitigate this is to check the spectral flatness (sometimes called tonality) of the signal and ignore the results if over a certain threshold. Spectral flatness is usually defined as the ratio of the geometric mean and arithmetic mean of the power spectrum. This will be revisited in a later chapter.

% !!!!!! ------------------------- Actually, this seems to happen sometimes, look into it.
\todo{The following is not entirely right, do something about it}
According to \cite{Smyth2019}, the HPS is also prone to getting octaves wrong, but that did not seem to be the case when testing preliminarily with guitar, piano and a voice.

% CFAR
\subsubsection{Constant False Alarm Rate}
% https://www.youtube.com/watch?v=BEg29UuZk6c
Constant False Alarm Rate (CFAR) is an algorithm that gives a dynamic threshold. It works by looking at some amount of reference cells that are beyond some gap around the target cell. For example for cells 5, 6, 7, 8, 9, 10, 11, 12, 13 the threshold given a gap size of 2 and reference cell count of 2 and the target is 9, the reference cells are 5, 6, 12 and 13. 7, 8, 10 and 11 are gap cells. The reference cells are then averaged (but other statistical methods can be used) and a bias may be applied. If the value of cell 9 is above the computed threshold, it's considered a target and noise if below. CFAR is typically used in conjuction with radar technology, but could just as well be used to discern the partials in a spectrum. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{./images/cfar.png}
    \caption{Diagram of the CFAR algorithm with 2 gap cells and 2 reference cells. The gap cells around the target point P are excluded but the reference cells in light blue are used for calculations. The reference cells are averaged which gives the threshold T for point P. As P is greater than T, P is regarded as a target.\label{fig:cfar}}
\end{figure}

To effectively use CFAR, the parameters must be carefully chosen. If the bias is too low, invalid targets may be considered targets and vice versa if the bias is too high. The gap and reference cells can be farly flexible but not too large as smaller harmonics may be considered noise compared to greater harmonics. The lowest pitch we want to estimate is around 50Hz and if we assume monophonic singing, the spectrum should mostly contain integer multiples of the fundemental frequency which means that no peaks should ever be closer than 50Hz. With 4Hz bins, we can freely chose up to 12 gap and reference cells and not have a harmonic interfere with the CFAR computations of another. 

As CFAR only finds the peaks, one problem with using it alone as a fundemental frequency estimator is that additional post processing is necessary. Since HPS already works well for peak picking, it is chosen as the primary algorithm for the fundamental frequency estimatotr. CFAR could however be used to give a rough number for peaks, which may be used to dynamically adjust the amount of iterations the HPS uses. If CFAR finds very few peaks or even just one as in the case of the electric piano in figure \ref{fig:pianoFreq}, the result could be used to completely change peak picker as HPS has issues with signals without harmonics.

% Problems with FFT, might move this to real-time
\subsection{Problems with the FFT}
One problem with the FFT that \cite{Gotsopoulos} addresses is the size of the frequency bins. This is also noted for the STFT, limiting how short the STFT can be for achieving sufficient frequency resolution \cite{Evans2012}. The FFT doesn't find precise frequencies, it finds bins of frequencies with the size $S/N$ where $S$ is the sampling rate and $N$ is the FFT buffer size. As frequency grows exponentially (doubles every octave), higher notes are spaced further apart. This means that for lower notes, the frequency bins need to be significantly smaller as shown in figure \ref{fig:fftBinSizeChart}. Base singers may need to go very low, around E2 (87Hz) and in order to avoid notes in this range to fall into the same bin, the window size needs to be very small.  

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{./images/fft_bin_size_chart.png}
    \caption{As the FFT window shrinks, different notes may fall into the same bin. Bars represent the bin sizes for various window sizes at 48kHz sampling rate and dashed lines show semitone intervals (the exact note values are irrelevant). For notes in the 4th octave, the window size can be smaller, but in the 2nd octave even 8192 samples is not enough to discern all pairs of semitones because the bins are larger than those semitone differences.\label{fig:fftBinSizeChart}}
\end{figure}

Base singers typically go as low as E2 and the difference between E2 and F2 is barely 5Hz which is smaller than the frequency bin of 8192 samples at 48kHz. They do not necessarily fall into the same bin but it would be safer to compute the FFT with a 16384 window size. This halves the frequency bin size and definitely accomodates differences even an octave lower. This introduces a significant amount of increased computation and is quite unnecessary as base singers do not realistically go much lower than E2. If the FFT implementation allows (many implementations require a power of 2), the window size should lie somewhere between 8192 and 16384 to lessen computations. For a 4Hz bin, $48000/x = 4 \iff x = 48000/4 = 12000$, 12000 samples would be enough to discern the lowest notes. 

This introduces another problem, which has implications for real-time pitch detection which is that collecting 12000 samples at 48kHz takes 0.25 seconds, which arguably is not real-time anymore. As the bin size is $B = S/W$ means that $W = S/B$ and the latency (time taken to fill the FFT window) $L = W/S = \frac{S/B}{S} = 1/B$, the bin size and latency are inversely proportional, both can not be minimized. For a bin size of 4, it will necessarily take 0.25s to collect the FFT samples. This follows that naively transforming audio makes real-time pitch detection for base singers impossible because the detection latency will inherently always be too long with sufficiently fine frequency resolution.

% Note timings
\subsection{Note timings}
How a piece of music is performed often deviates a lot from how it was written. There are several reasons for this. For one, sheet music is not an algorithm, it contains many things that are up for interpretation, most notably when it comes to pace and dynamics. When a musician encounters the tempo marking \textit{andante}, they just have to feel what it means because there does not seem to be any consensus on what \textit{andante} means. Most sources give a range, but even the ranges are different between sources. Another term, \textit{accelerando} simply means to accelerate. How much is not specified. Musicians may ignore them completely and even make them up, to introduce their own touch into the performance. In the more simpler case, some sections may just be dragged as the musician or conductor wants. Figure \ref{fig:performance-sheet} visualizes two time series using colors instead of magnitude, the one above is the precise note timing as the sheet music states. The sheet also states to be played in \textit{allegro} and the tempo isn't modified at any time according to the sheet. The time series below is one performance of the piece. The performance was timed to be around 120 BPM (which indeed is \textit{allegro}) and so the performance was sampled at this tempo. Figure \ref{fig:performance-sheet} reveals how the pace in the rendition deviates from the sheet. It should be easy to see a strong similarity between the two. They are not quite identical, but their structure is identical and the substructures are similar. 

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{./images/performance-sheet.png}
    \caption{The time series of a piece as the sheet music suggests (above) and of a performance of said piece (below)\label{fig:performance-sheet}}
\end{figure}

\subsubsection{Time series similarity}
The similarity of two time series can be measured with something called Dynamic Time Warping. 

% Narrowing the problem
\subsection{Real-time pitch detection}
The focus of this work is on real-time pitch detection, specifically for the purpose of rating and aiding a user in practicing their singing. To clarify, real-time means that the input of the system is a stream and the system processes small chunks as they come up and forget anything that it has already processed just like a human would listen to a short window in time, decide the pitch for that window and moving on, not caring about the previous windows and not knowing anything of upcoming windows. The acceptable latency of "real-time" is contextual. For rendering, it should be around 33 milliseconds to achieve 30 frames per second, where as for real-time tracking in logistics, it's probably fine to update every minute or even hour. 

What real-time is in this context can be debated and studied further, but the goal is just that it's fast enough so that user can react to their mistakes. According to humanbenchmark.com which tests, among other things, reaction time, the mean reaction time is 284ms based on their collected 81M benchmarks \cite{HumanBenchmark2025}. The Human Benchmark benchmark is based on visual stimuli but it's noted that reaction to auditory stimulus is slightly faster \cite{SheltonKumar2010}. The goal for this work is significantly below the mean visual reaction time at around 100ms. Even if humans can react to auditory stimulus faster than this, as the purpose is to give live feedback to the user on their own singing rather than having the user react to someone else's singing, the bottleneck is the user's reaction speed to the feedback any way. 

% Zero padding
\subsubsection{Zero-padding}
One challenge with real-time pitch detection that emerged from the problem with the size of frequency bins was that for detecting lower base notes, the window size needed to be around 12000. However collecting 12000 samples at 48kHz takes 0.25s, which arguably is not real-time anymore. It takes 0.33s if for any reason the window size needs to be 16384. If the window size is made smaller, the resulting FFT will have larger bins than what is necessary for detecting lower frequencies. 

A popular approach to increasing frequency resolutions is to pad the end of the signal with zeros. Zero-padding makes the signal longer and while it necessarily introduces frequency components (more bins with a longer input signal), the peaks remain intact. A larger FFT window, while increasing computational complexity, allows for better frequency resolution. At 48kHz, 16000 samples are enough to achieve a 3Hz frequency bin. If out of these 16000, 10000 are zeros and 6000 legitimate datapoints, the FFT window can be "filled" in a mere 125ms which is a more acceptable latency for real-time detection. Figure \ref{fig:zeropadSpectrum} shows how zero-padding affects the spectrum.

\begin{figure}[ht]
    \centering
    \includegraphics[width=\textwidth]{./images/zero_pad_spectrum.png}
    \caption{Spectrum contents of a signal consisting of 260, 440 and 880 frequency components. While the zero-padding introduces a small amount of frequency data around the peaks, the peaks themselves are not affected. \label{fig:zeropadSpectrum}}
\end{figure}

\subsubsection{Reducing sampling rate and window size}
As the bin size is the ratio of the sampling rate to number of samples, $B = S/N$, reducing both $N$ and $S$ keeps the bin size small. If only the window size is reduced, the bins grow in size and frequency resolution is lost. With zero-padding fixing the latency issue, why not reduce the sample rate as well to ease computational load? Apart from the system limitations of sampling rate, like the W3C web audio specifications only guaranteeing 8000 to 96000Hz, another issue is aliasing. Aliasing in short is misrepresentation of higher frequencies as lower frequencies. 

The Nyquist-Shannon sampling theorem states that there is no loss of imformation when sampling at 2B hertz if the signal contains frequency components that are less than B hertz. It thus implies that if we do lower the sampling rate, there will be some ambiguity about about the highest frequencies. For example, for the most common sampling rate 44100Hz, the maximum frequency that can be represented without ambiguity is 22050Hz. This is also called the Nyquist frequency. In other words, the Nyquist-Shannon theorem says that no information is lost as long as all frequency components are below the Nyquist frequency. 22050 being just above all frquencies humans can hear is a key factor for 44100Hz being the common default. 

As the pitch detector in this work has no reason to play back the sound, we can do whatever to with the waveform as long as no crucial information is lost or skewed. Compared to the lowest male voices, there's seems to be less consensus on the highest female voice, but a pitch higher than G6 at around 1567Hz see\ref{fig:performance-sheet}ms to be uncommon. A more typical is C6 at around 1046Hz. To be on the safe side, the 1567Hz can still be considered a valid requirement. If the pitch detector needs to detect a 1567Hz frequency, the sampling rate needs to be no smaller than 3134Hz. However, the human voice is rich in harmonics, if these are aliased, they will be presented as lower notes which in the best case would get the octave wrong and in the worst case get completely wrong notes.

The sampling theoren clearly states what we can and can not do with the signal but it all depends on the harmonics of voices. Clearly, the sampling rate can not be 3134Hz because a significant amount of the relevant information will be in the harmonics above 1567Hz, but from limited testing, the harmonics certainly don't approach the 22050 Nyquist frequency.  Even for female voices, the spectral content seems to be very weak above 10000Hz and almost indistinguishable from noise above 15000Hz. The sampling rate thus could probably be lowered to 30kHz or even 20kHz which would allow an FFT window size of 8192. This could be considered and tested if system resources were an issue, but a modern laptop is performant enough for a 16394 point FFT to not slow down the program.

\subsection{Finding nearest note}
%https://newt.phys.unsw.edu.au/jw/notes.html
Western tuning is done in equal temperaments, meaning the octave is evenly divided in 12. Using A4 (440Hz) as a reference, one can calculate every semitone's frequency with a signed index from A4 using the following function. 
$$f(i) = 2^{i/12}*440$$
For example the -2nd semitone using the formula is approximately G4 which indeed is 2 semitones under A4. To avoid negative indices, they can be shifted by some number. A4 is assigned the MIDI number 69 so this will be the number the scale gets shifted by. So to find the frequency of the i-th MIDI number the following function can be used.
$$f(i) = 2^{\frac{i-69}{12}}*440$$
To find a MIDI number based on frequency, the function can be rewritten as a function of $f$.
$$M(f) = 12*\log_2(f/440)+69$$
While the MIDI numbers are discrete, the derived formula interpolates between the MIDI numbers. If the input is a number that isn't a semitone, like 450, the result is $M(450) \approx 69.4$. This can be rounded to the nearest integer.

The function that will be used in the pitch detector is
$$M(f) = round(12*\log_2(f/440)+69)$$

\subsubsection{Time keeping}
How a piece is performed may deviate significantly from how it is written down by the author. If the purpose was offline processing, where the entire signal is given and the system processes it as fast as it can, DTW and other related processing tools may be used to rate the similarity of the performance and the how it's written to conclude how accurate the user's singing was. Here, the input is the user's entire performance/practice and they get back the complete analysis in one go. This won't necessarily take long because the performance penalty of the analysis is negligeable.

However since the goal is to give live feedback to the user, to analyse any note, the system needs to know at all times what note it should be expecting. One easy approach is to just force the user into a steady tempo and perhaps have a metronome to help them keep the tempo. This way, the system can at a constant pace, iterate through all the expected notes and compare whatever it has to whatever it gets. This is both easy to implement and beneficial for the user because even though there is artistic freedom in music, tempo and keeping relative note lengths correct is still vital. 

While the time series in \ref{fig:performance-sheet} illustrate a important point about relative time in music, the application will solve this by forcing the user to keeping a constant tempo which effectively aligns actual and expected notes, unless the user makes an error in time keeping. DTW will partially be used in testing the pitch detector because of recordings that were provided for this work by Finlands svenska manssångarförbund (FSM), an alliance of finland-swedish men's choirs. 
